{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "390d6cb5",
   "metadata": {},
   "source": [
    "## Session 02: Optimization of the Linear Regression Model\n",
    "\n",
    "### Goran S. Milovanović, Phd\n",
    "\n",
    "Feedback should be send to [goran.milovanovic@datakolektiv.com](mailto:goran.milovanovic@datakolektiv.com). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "849cd1ca",
   "metadata": {},
   "source": [
    "![](DK_ML_ConfPalic2023.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "939d42bc",
   "metadata": {},
   "source": [
    "### Lecturers\n",
    "\n",
    "[Goran S. Milovanović, PhD, DataKolektiv, Chief Scientist & Owner](https://www.linkedin.com/in/gmilovanovic/)\n",
    "\n",
    "[Aleksandar Cvetković, PhD, DataKolektiv, Consultant](https://www.linkedin.com/in/alegzndr/)\n",
    "\n",
    "[Ilija Lazarević, MA, DataKolektiv, Consultant](https://www.linkedin.com/in/ilijalazarevic/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79c48d32",
   "metadata": {},
   "source": [
    "![](DK_Logo_100.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "582b4761",
   "metadata": {},
   "source": [
    "![](../img/DK_Logo_100.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d08d2dbd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b22faf42",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a720d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - lib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# - parameters\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "sns.set_theme()\n",
    "# - rng\n",
    "rng = np.random.default_rng()\n",
    "# - plots\n",
    "plt.rc(\"figure\", figsize=(8, 6))\n",
    "plt.rc(\"font\", size=14)\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "# - directory tree\n",
    "data_dir = os.path.join(os.getcwd(), '_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc8e5438",
   "metadata": {},
   "source": [
    "## 0. Covariance and Correlation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fef67ce",
   "metadata": {},
   "source": [
    "### Covariance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2caf2caf",
   "metadata": {},
   "source": [
    "Given two random variables (RVs), $X$ and $Y$, their (sample) covariance is given by:\n",
    "\n",
    "$$cov(X,Y) = E[(X-E[X])(Y-E[Y])] = \\frac{(X-\\bar{X})(Y-\\bar{Y})}{N-1}$$\n",
    "\n",
    "where $E[X]$ denotes the *expectation* (the *mean*, if you prefer) of $X$, $\\bar{X}$ is the mean of $X$, $\\bar{Y}$ is the mean of $Y$, and $N$ is the sample size.\n",
    "\n",
    "\n",
    "**Some properties of covariance.**\n",
    "\n",
    "**Bilinearity.** Covariance is a bilinear function, meaning that it is linear in each argument separately. That is, for any constants a and b, we have:\n",
    "\n",
    "$$cov(aX+b, Y) = acov(X,Y) + bcov(Y,Y)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$cov(X, aY+b) = acov(X,Y) + bcov(X,X)$$\n",
    "\n",
    "**Symmetry.** Covariance is a symmetric measure, meaning that the covariance between X and Y is the same as the covariance between Y and X. That is:\n",
    "\n",
    "$$cov(X,Y) = cov(Y,X)$$\n",
    "\n",
    "**Independence.** If X and Y are independent random variables, then their covariance is zero. That is:\n",
    "\n",
    "$$cov(X,Y) = E[(X-\\bar{X})(Y-\\bar{Y})] = E[X-\\bar{X}]E[Y-\\bar{Y}] = 0$$\n",
    "\n",
    "The **covariance of a variable with itself is equal to its variance**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bb6bccd",
   "metadata": {},
   "source": [
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23231e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(x, y):\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"x and y must be of same size.\")\n",
    "    # the means of x and y\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)    \n",
    "    # the deviations of x and y from their means\n",
    "    x_dev = x - x_mean\n",
    "    y_dev = y - y_mean\n",
    "    # the covariance of x and y\n",
    "    cov = np.mean(x_dev * y_dev)\n",
    "    # ouput\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f831b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = norm.rvs(loc=1.7, scale=.6, size=1000)\n",
    "sample2 = norm.rvs(loc=2.85, scale=.6, size=1000)\n",
    "covariance(sample1, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = norm.rvs(loc=1.7, scale=.6, size=100000)\n",
    "# linear scaling of sample1\n",
    "a, b = .75, 2\n",
    "sample2 = a*sample1+b\n",
    "# add random noise to sample2\n",
    "eps = norm.rvs(loc=0, scale=0.27, size=100000)\n",
    "sample2 = sample2 + eps\n",
    "samples = pd.DataFrame({'x':sample1, 'y':sample2})\n",
    "samples.plot.scatter(x='x', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance(samples['x'], samples['y'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99888e04",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6935853",
   "metadata": {},
   "source": [
    "**Pearson's coefficient of correlation** is nothing else than a covariance between $X$ and $Y$ upon their *standardization*. The standardization of a RV - widely known as a variable *z-score* - is obtained upon subtracting all of its values from the mean, and dividing by the standard deviation; for the **i**-th observation of $X$\n",
    "\n",
    "$$z(x_i) = \\frac{x_i-\\bar{X}}{\\sigma}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69bc6e7e",
   "metadata": {},
   "source": [
    "Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(x):\n",
    "    # the mean and standard deviation of x\n",
    "    x_mean = np.mean(x)\n",
    "    x_std = np.std(x)    \n",
    "    # the z-score of x\n",
    "    z = (x - x_mean) / x_std\n",
    "    # output\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39056d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['x_z'] = z_score(samples['x'])\n",
    "samples['y_z'] = z_score(samples['y'])\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef805a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(samples['x_z']))\n",
    "print(np.std(samples['x_z'], ddof=1))\n",
    "print(np.mean(samples['y_z']))\n",
    "print(np.std(samples['y_z'], ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance(samples['x_z'], samples['y_z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(samples['x_z'], samples['y_z'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fdb5b72",
   "metadata": {},
   "source": [
    "Right. There are many formulas that compute `r`, the correlation coefficient; however, understanding that is simply the covariance of standardized RVs is essential. Once you know to standardize the variables and how to compute covariance (and that is easy), you don't need to care about expressions like:\n",
    "\n",
    "$$r_{XY} = \\frac{N\\sum{XY}-(\\sum{X})(\\sum{Y})}{\\sqrt{[N\\sum{X^2}-(\\sum{X})^2][N\\sum{Y^2}-(\\sum{Y})^2]}}$$\n",
    "\n",
    "This and similar expressions are good, and especially for two purposes: first, they will compute the desired value of the correlation coefficient in the end, that's for sure, and second, writing them up in really helps mastering $\\LaTeX$. Besides these roles they play, there is really nothing essentially important in relation to them.\n",
    "\n",
    "Somewhat easier to remember:\n",
    "\n",
    "$$r_{XY} = \\frac{cov(X,Y)}{\\sigma(X)\\sigma(Y)}$$\n",
    "\n",
    "i.e. correlation is the covariance of $X$ and $Y$, divided by the product of their standard deviations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9a92261",
   "metadata": {},
   "source": [
    "## 1. Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162174b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Setup - importing the libraries\n",
    "\n",
    "# - supress those annoying 'Future Warning'\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# - data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# - os\n",
    "import os\n",
    "\n",
    "# - ml\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "# - visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# - parameters\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "sns.set_theme()\n",
    "# - rng\n",
    "rng = np.random.default_rng()\n",
    "# - plots\n",
    "plt.rc(\"figure\", figsize=(8, 6))\n",
    "plt.rc(\"font\", size=14)\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "# - directory tree\n",
    "data_dir = os.path.join(os.getcwd(), '_data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5f1c549",
   "metadata": {},
   "source": [
    "We will use the `Fish.csv` data set in this session. You can grab it from [Kaggle: fish-market](https://www.kaggle.com/datasets/aungpyaeap/fish-market). Please place the `Fish.csv` data set into your `_data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - loading the dataset\n",
    "# - Kaggle: https://www.kaggle.com/datasets/aungpyaeap/fish-market\n",
    "# - place it in your _data/ directory\n",
    "fish_data = pd.read_csv(os.path.join(data_dir, 'Fish.csv'))\n",
    "fish_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dbb02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3464dbde",
   "metadata": {},
   "source": [
    "### Target: predict Weight from Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frame = fish_data[['Height', 'Weight']]\n",
    "model_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting the linear model to the data\n",
    "linear_model = smf.ols(formula='Weight ~ Height', data=model_frame).fit()\n",
    "linear_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9a0aa7f",
   "metadata": {},
   "source": [
    "Linear model has the form\n",
    "\n",
    "$$y = \\beta_1 x + \\beta_0 + \\varepsilon,$$\n",
    "\n",
    "where\n",
    "- $y$ - the true value of the *target variable*\n",
    "- $\\beta_1$ - the *slope* of the model\n",
    "- $\\beta_0$ - the *intercept* of the model\n",
    "- $\\varepsilon$ - the *residual*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "911c0216",
   "metadata": {},
   "source": [
    "The predicted value $\\hat{y}$ of the target variable is computed via Liner regression via\n",
    "\n",
    "$$\\hat{y} = \\beta_1 x + \\beta_0.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model's parameters; Height represent the slope k\n",
    "linear_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - predicting the value using model's formula and parameters\n",
    "model_frame['Predicted Values'] = linear_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ed38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - calculating the residuals - the difference between the true and predicted values\n",
    "model_frame['Residuals'] = linear_model.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552714cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - some statistics on the residuals\n",
    "model_frame['Residuals'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plotting the true data, predicted values and the prediction line\n",
    "sns.regplot(data=model_frame, x='Height', y='Weight', ci=0, line_kws={'color':'red'})\n",
    "sns.scatterplot(data=model_frame, x='Height', y='Predicted Values', color='red', s=50)\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.title('Weight ~ Hight', fontsize=14);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b554315e",
   "metadata": {},
   "source": [
    "Ok, `statsmodels` can do it; how do we find out about the optimal values of $\\beta_0$ and $\\beta_1$?\n",
    "Let's build ourselves a function that (a) tests for some particular values of $\\beta_0$ and $\\beta_1$ for a particular regression problem (i.e. for a particular dataset) and returns the model error.\n",
    "\n",
    "The model error? Oh. Remember the residuals:\n",
    "\n",
    "$$\\epsilon_i = y_i - \\hat{y_i}$$\n",
    "\n",
    "where $y_i$ is the observation to be predicted, and $\\hat{y_i}$ the actual prediction?\n",
    "\n",
    "Next we do something similar to what happens in the computation of variance, square the differences:\n",
    "\n",
    "$$\\epsilon_i^2 = (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "and define the model error for all observations to be **the sum of squares**:\n",
    "\n",
    "$$SSE = \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "Obviously, the lower the $SSE$ - the Sum of Squared Error - the better the model! Here's a function that returns the SSE for a given data set (with two columns: the predictor and the criterion) and a choice of parameters $\\beta_0$ and $\\beta_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - sse function\n",
    "def lg_sse(pars):\n",
    "    # - pick up the parameters\n",
    "    beta_0 = pars[0]\n",
    "    beta_1 = pars[1]\n",
    "    # - predict from parameters\n",
    "    preds = beta_0+beta_1*fish_data['Height']\n",
    "    # - compute residuals\n",
    "    residuals = fish_data['Weight']-preds\n",
    "    # - square the residuals\n",
    "    residuals = residuals**2\n",
    "    # - sum of squares\n",
    "    residuals = residuals.sum()\n",
    "    # - out:\n",
    "    return residuals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ab68bf",
   "metadata": {},
   "source": [
    "Test `lg_sse()` now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138105b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = [-144.385971, 60.496351]\n",
    "print(lg_sse(pars))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "677316aa",
   "metadata": {},
   "source": [
    "Check via `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f051212",
   "metadata": {},
   "outputs": [],
   "source": [
    "(linear_model.resid**2).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3592d96b",
   "metadata": {},
   "source": [
    "Method A. Random parameter space search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e01f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = np.random.uniform(low=-200, high=200, size=10000)\n",
    "beta_1 = np.random.uniform(low=-200, high=200, size=10000)\n",
    "random_pars = pd.DataFrame({'beta_0':beta_0, 'beta_1':beta_1})\n",
    "random_pars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aade2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "for i in range(random_pars.shape[0]):\n",
    "    pars = [random_pars['beta_0'][i],random_pars['beta_1'][i]]\n",
    "    sse.append(lg_sse(pars))\n",
    "random_pars['sse'] = sse\n",
    "random_pars.sort_values('sse', ascending=True, inplace=True)\n",
    "random_pars.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d36cf775",
   "metadata": {},
   "source": [
    "Check with `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf68090",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "def2f371",
   "metadata": {},
   "source": [
    "Not bad, how about 100,000 random pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = np.random.uniform(low=-200, high=200, size=100000)\n",
    "beta_1 = np.random.uniform(low=-200, high=200, size=100000)\n",
    "random_pars = pd.DataFrame({'beta_0':beta_0, 'beta_1':beta_1})\n",
    "sse = []\n",
    "for i in range(random_pars.shape[0]):\n",
    "    pars = [random_pars['beta_0'][i],random_pars['beta_1'][i]]\n",
    "    sse.append(lg_sse(pars))\n",
    "random_pars['sse'] = sse\n",
    "random_pars.sort_values('sse', ascending=True, inplace=True)\n",
    "random_pars.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f038ff03",
   "metadata": {},
   "source": [
    "Method B. Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0_vals = np.linspace(-200,200,100)\n",
    "beta_1_vals = np.linspace(-200,200,100)\n",
    "grid = np.array([(beta_0, beta_1) for beta_0 in beta_0_vals for beta_1 in beta_1_vals])\n",
    "grid = pd.DataFrame(grid)\n",
    "grid = grid.rename(columns={0: \"beta_0\", 1: \"beta_1\"})\n",
    "grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c92bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "for i in range(grid.shape[0]):\n",
    "    pars = [grid['beta_0'][i],grid['beta_1'][i]]\n",
    "    sse.append(lg_sse(pars))\n",
    "grid['sse'] = sse\n",
    "grid.sort_values('sse', ascending=True, inplace=True)\n",
    "grid.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19ff9076",
   "metadata": {},
   "source": [
    "A grid more dense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0_vals = np.linspace(-200,200,1000)\n",
    "beta_1_vals = np.linspace(-200,200,1000)\n",
    "grid = np.array([(beta_0, beta_1) for beta_0 in beta_0_vals for beta_1 in beta_1_vals])\n",
    "grid = pd.DataFrame(grid)\n",
    "grid = grid.rename(columns={0: \"beta_0\", 1: \"beta_1\"})\n",
    "sse = []\n",
    "for i in range(grid.shape[0]):\n",
    "    pars = [grid['beta_0'][i],grid['beta_1'][i]]\n",
    "    sse.append(lg_sse(pars))\n",
    "grid['sse'] = sse\n",
    "grid.sort_values('sse', ascending=True, inplace=True)\n",
    "grid.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b81f5347",
   "metadata": {},
   "source": [
    "Check with `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bc31f83",
   "metadata": {},
   "source": [
    "Method C. Optimization (the real thing)\n",
    "\n",
    "The Method of Least Squares"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d99b67f",
   "metadata": {},
   "source": [
    "Here is the real thing. \n",
    "\n",
    "- **Question.** What do we really need to do to find the optimal $\\beta_0$, $\\beta_1$ parameters of the Simple Linear Regression Model?\n",
    "- **Answer.** Of course, we need to find $\\beta_0$, $\\beta_1$ **at the minimum of the $SSE$ - the error - function.**\n",
    "\n",
    "And how do we do that?\n",
    "\n",
    "Well, in a particular case of a (Simple or Multiple) Linear Regression Model, it turns out that is possible to provide an analytical solution for all model parameteres that minimize the model $SSE$ (error) function. It takes some time work through the partial derivates of $SSE$ in respect to each model parameter, but it works in the end.\n",
    "\n",
    "*But finding analytical solutuion will not work for just any statistical model.*\n",
    "\n",
    "Now, imagine that we have an algorithm - call it an **optimization algorithm ** - that can find the parameters that minimize a respective function. Indeed we have such an algorithm. Indeed we have many different such algorithms, developed by experts in the very, very alive and complicated branch of mathematics called Optimization Theory. We will put one such algorithm - the famed Nelder-Mead Simplex Method - to work in order to minimize $SSE$ in respect to $\\beta_0$, $\\beta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# - sse function\n",
    "def lg_sse(pars, data):\n",
    "    # - pick up the parameters\n",
    "    beta_0 = pars[0]\n",
    "    beta_1 = pars[1]\n",
    "    # - predict from parameters\n",
    "    preds = beta_0+beta_1*fish_data['Height']\n",
    "    # - compute residuals\n",
    "    residuals = fish_data['Weight']-preds\n",
    "    # - square the residuals\n",
    "    residuals = residuals**2\n",
    "    # - sum of squares\n",
    "    residuals = residuals.sum()\n",
    "    # - out:\n",
    "    return residuals\n",
    "\n",
    "# - initial (random) parameter values\n",
    "init_beta_0 = np.random.uniform(low=-15, high=15, size=1)\n",
    "init_beta_1 = np.random.uniform(low=-15, high=15, size=1)\n",
    "init_pars = [init_beta_0, init_beta_1]\n",
    "\n",
    "# - optimize w. Nelder-Mead\n",
    "optimal_model = sp.optimize.minimize(\n",
    "    # - fun(parameters, args)\n",
    "    fun=lg_sse,\n",
    "    args = (fish_data), \n",
    "    x0 = init_pars, \n",
    "    method='Nelder-Mead')\n",
    "\n",
    "# - optimal parameters\n",
    "optimal_model.x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f2a626",
   "metadata": {},
   "source": [
    "Check against `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1c29ac9",
   "metadata": {},
   "source": [
    "Final value of the objective function (the model SSE, indeed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be948418",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_sse(pars=optimal_model.x, data=fish_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae9474d2",
   "metadata": {},
   "source": [
    "Check against `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.ssr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a710428a",
   "metadata": {},
   "source": [
    "Error Surface Plot: The Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0_vals = np.linspace(-160,-120,200)\n",
    "beta_1_vals = np.linspace(40,80,200)\n",
    "grid = np.array([(beta_0, beta_1) for beta_0 in beta_0_vals for beta_1 in beta_1_vals])\n",
    "grid = pd.DataFrame(grid)\n",
    "grid = grid.rename(columns={0: \"beta_0\", 1: \"beta_1\"})\n",
    "sse = []\n",
    "for i in range(grid.shape[0]):\n",
    "    pars = [grid['beta_0'][i],grid['beta_1'][i]]\n",
    "    sse.append(lg_sse(pars, fish_data))\n",
    "grid['sse'] = sse\n",
    "grid.sort_values('sse', ascending=True, inplace=True)\n",
    "grid.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46e712ec",
   "metadata": {},
   "source": [
    "This the function that we have minimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fdd4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "\n",
    "# - Mesh3d: Objective Function\n",
    "fig = go.Figure(data=[go.Mesh3d(\n",
    "    x=grid['beta_0'], \n",
    "    y=grid['beta_1'], \n",
    "    z=grid['sse'], \n",
    "    color='lightblue', \n",
    "    opacity=0.50)])\n",
    "fig.update_layout(scene = dict(\n",
    "                    xaxis_title='Beta_0',\n",
    "                    yaxis_title='Beta_1',\n",
    "                    zaxis_title='SSE'),\n",
    "                    width=700,\n",
    "                    margin=dict(r=20, b=10, l=10, t=10))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08048105",
   "metadata": {},
   "source": [
    "Back to statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Pearson's correlation (R-value) coefficient and R^2\n",
    "print(f\"Pearson's correlation (R-value): {round(np.sqrt(linear_model.rsquared), 4)}\")\n",
    "print(f\"Coefficient of determination (R^2): {round(linear_model.rsquared, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be603544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - p-values of the model's parameters\n",
    "print(f\"p-values: \\n{linear_model.pvalues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334790f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predicting new data\n",
    "\n",
    "predictions = pd.DataFrame(columns=['Height', 'Weight'])\n",
    "\n",
    "# - sampling the new data from the normal distribution with the mean and std parameters taken from the original data\n",
    "new_fish_height = rng.normal(loc=model_frame['Height'].mean(), scale=model_frame['Height'].std(), size=100)\n",
    "# - clipping the negative values \n",
    "new_fish_height = np.clip(new_fish_height, a_min=0, a_max=np.infty)\n",
    "predictions['Height'] = new_fish_height\n",
    "\n",
    "# - predicting the heights on the new data using the linear model\n",
    "new_fish_weight = linear_model.predict(predictions['Height'])\n",
    "\n",
    "# - displaying the new data and the corresponding predictions\n",
    "predictions['Weight'] = new_fish_weight\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ee583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plotting the predictions\n",
    "sns.lineplot(data=predictions, x='Height', y='Weight', color='red')\n",
    "sns.scatterplot(data=predictions, x='Height', y='Weight', color='red', s=50)\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.title('Predictions', fontsize=14);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3de018d0",
   "metadata": {},
   "source": [
    "## 2. Multiple Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9155a70",
   "metadata": {},
   "source": [
    "The formula for the Multiple Linear Regression Model has the form\n",
    "\n",
    "$$y = \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_kx_k + \\beta_0 + \\varepsilon,$$\n",
    "\n",
    "where \n",
    "\n",
    "- $y$ - the true value of target variable\n",
    "- $x_1, x_2, \\ldots, x_k$ - the predictors' values\n",
    "- $\\beta_1, \\beta_2, \\ldots, \\beta_k$ - model's parameters for the predictors\n",
    "- $\\beta_0$ - the intercept of the model\n",
    "- $\\varepsilon$ - the residual\n",
    "\n",
    "To predict a value $\\hat{y}$ of the target variable via Multiple Linear Regression, we use\n",
    "\n",
    "$$\\hat{y} = \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_kx_k + \\beta_0.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afdfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Composing the fomula of the model\n",
    "\n",
    "predictors = fish_data.columns.drop('Weight')\n",
    "print(predictors)\n",
    "\n",
    "# - right side of the formula\n",
    "formula = ' + '.join(predictors)\n",
    "\n",
    "# - left side of the formula\n",
    "formula = 'Weight ~ ' + formula\n",
    "\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd206d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting the linear model to the data\n",
    "linear_model = smf.ols(formula=formula, data=fish_data).fit()\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model's parameters\n",
    "linear_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce163d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - predicting the values using linear model's formula and parameters\n",
    "model_frame['Predicted Weight'] = linear_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bf728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plotting the true values vs predicted values\n",
    "# - the identity line (y=x) shows how good is the prediction - the closer the datapoint to the line, the better\n",
    "sns.scatterplot(data=model_frame, x='Predicted Weight', y='Weight')\n",
    "sns.lineplot(x=np.arange(-500, 2000), y=np.arange(-500, 2000), color='k')\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.title('Model Predictions \\n Note: identity line', fontsize=14);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0efa650e",
   "metadata": {},
   "source": [
    "### Further Reading:\n",
    "\n",
    "- [https://www.khanacademy.org/math/statistics-probability/probability-library](https://www.khanacademy.org/math/statistics-probability/probability-library)\n",
    "\n",
    "- [Random variables | Statistics and probability | Math | Khan Academy]( https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library)\n",
    "\n",
    "- [Brandon Foltz, Statistics PL15 - Multiple Linear Regression Playlist](https://www.youtube.com/playlist?list=PLIeGtxpvyG-IqjoU8IiF0Yu1WtxNq_4z-)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2b89804",
   "metadata": {},
   "source": [
    "DataKolektiv, 2022/23.\n",
    "\n",
    "[hello@datakolektiv.com](mailto:goran.milovanovic@datakolektiv.com)\n",
    "\n",
    "![](../img/DK_Logo_100.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efa1e1ec",
   "metadata": {},
   "source": [
    "<font size=1>License: [GPLv3](https://www.gnu.org/licenses/gpl-3.0.txt) This Notebook is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This Notebook is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this Notebook. If not, see http://www.gnu.org/licenses/.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
